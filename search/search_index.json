{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"play-with-torch Repository for playing the computer vision apps: Vehicle classification on Raspberry Pi. Tools Tested Hardware RasberryPi 4 Model B here , RAM: 4 GB and Processor 4-core @ 1.5 GHz microSD Card 64 GB 5M USB Retractable Clip 120 Degrees WebCam Web Wide-angle Camera Laptop U7 Mini or Raspi Camera Tested Software Ubuntu Desktop 20.10 aarch64 64 bit , install on RasberriPi 4 PyTorch: torch 1.6.0 aarch64 and torchvision 0.7.0 aarch64 Python min. ver. 3.6 (3.8 recommended) Install the prerequisites Install packages $ sudo apt install build-essential make cmake git python3-pip libatlas-base-dev $ sudo apt install libssl-dev $ sudo apt install libopenblas-dev libblas-dev m4 python3-yaml $ sudo apt install libomp-dev make swap space to 2048 MB $ free -h $ sudo swapoff -a $ sudo dd if=/dev/zero of=/swapfile bs=1M count=2048 $ sudo mkswap /swapfile $ sudo swapon /swapfile $ free -h Install torch 1.6.0 $ pip3 install torch-1.6.0a0+b31f58d-cp38-cp38-linux_aarch64.whl Folder Structure play-with-torch/ \u251c\u2500\u2500 inference.py - main script to inference model \u251c\u2500\u2500 train.py - main script to start training \u251c\u2500\u2500 test.py - evaluation of trained model \u2502 \u251c\u2500\u2500 config.json - holds configuration for training \u251c\u2500\u2500 parse_config.py - class to handle config file and cli options \u2502 \u251c\u2500\u2500 docs/ - for documentation \u2502 \u2514\u2500\u2500 play-with-torch.tex \u2502 \u251c\u2500\u2500 templates/ - for serving model on Flask \u2502 \u2514\u2500\u2500 index.html \u2502 \u251c\u2500\u2500 base/ - abstract base classes \u2502 \u251c\u2500\u2500 base_data_loader.py \u2502 \u251c\u2500\u2500 base_model.py \u2502 \u2514\u2500\u2500 base_trainer.py \u2502 \u251c\u2500\u2500 data_loader/ - anything about data loading goes here \u2502 \u2514\u2500\u2500 data_loaders.py \u2502 \u251c\u2500\u2500 data/ - default directory for storing input data \u2502 \u251c\u2500\u2500 model/ - models, losses, and metrics \u2502 \u251c\u2500\u2500 model.py \u2502 \u251c\u2500\u2500 metric.py \u2502 \u2514\u2500\u2500 loss.py \u2502 \u251c\u2500\u2500 saved/ \u2502 \u251c\u2500\u2500 models/ - trained models are saved here \u2502 \u2514\u2500\u2500 log/ - default logdir for tensorboard and logging output \u2502 \u251c\u2500\u2500 trainer/ - trainers \u2502 \u2514\u2500\u2500 trainer.py \u2502 \u251c\u2500\u2500 logger/ - module for tensorboard visualization and logging \u2502 \u251c\u2500\u2500 visualization.py \u2502 \u251c\u2500\u2500 logger.py \u2502 \u2514\u2500\u2500 logger_config.json \u2502 \u2514\u2500\u2500 utils/ - small utility functions \u251c\u2500\u2500 util.py \u2514\u2500\u2500 ... Usage $ git clone https://github.com/mheriyanto/play-with-torch.git $ cd play-with-torch $ python3 inference.py --source /dev/video0 TO DO [ ] Implement Unit-Test: Test-Driven Development (TDD) Credit to Share PyTorch binaries built for Raspberry Pi Reference Yunjey Choi - PyTorch Tutorial for Deep Learning Researchers here Victor Huang - PyTorch Template Project ( here )","title":"Home"},{"location":"#play-with-torch","text":"Repository for playing the computer vision apps: Vehicle classification on Raspberry Pi.","title":"play-with-torch"},{"location":"#tools","text":"","title":"Tools"},{"location":"#tested-hardware","text":"RasberryPi 4 Model B here , RAM: 4 GB and Processor 4-core @ 1.5 GHz microSD Card 64 GB 5M USB Retractable Clip 120 Degrees WebCam Web Wide-angle Camera Laptop U7 Mini or Raspi Camera","title":"Tested Hardware"},{"location":"#tested-software","text":"Ubuntu Desktop 20.10 aarch64 64 bit , install on RasberriPi 4 PyTorch: torch 1.6.0 aarch64 and torchvision 0.7.0 aarch64 Python min. ver. 3.6 (3.8 recommended)","title":"Tested Software"},{"location":"#install-the-prerequisites","text":"Install packages $ sudo apt install build-essential make cmake git python3-pip libatlas-base-dev $ sudo apt install libssl-dev $ sudo apt install libopenblas-dev libblas-dev m4 python3-yaml $ sudo apt install libomp-dev make swap space to 2048 MB $ free -h $ sudo swapoff -a $ sudo dd if=/dev/zero of=/swapfile bs=1M count=2048 $ sudo mkswap /swapfile $ sudo swapon /swapfile $ free -h Install torch 1.6.0 $ pip3 install torch-1.6.0a0+b31f58d-cp38-cp38-linux_aarch64.whl","title":"Install the prerequisites"},{"location":"#folder-structure","text":"play-with-torch/ \u251c\u2500\u2500 inference.py - main script to inference model \u251c\u2500\u2500 train.py - main script to start training \u251c\u2500\u2500 test.py - evaluation of trained model \u2502 \u251c\u2500\u2500 config.json - holds configuration for training \u251c\u2500\u2500 parse_config.py - class to handle config file and cli options \u2502 \u251c\u2500\u2500 docs/ - for documentation \u2502 \u2514\u2500\u2500 play-with-torch.tex \u2502 \u251c\u2500\u2500 templates/ - for serving model on Flask \u2502 \u2514\u2500\u2500 index.html \u2502 \u251c\u2500\u2500 base/ - abstract base classes \u2502 \u251c\u2500\u2500 base_data_loader.py \u2502 \u251c\u2500\u2500 base_model.py \u2502 \u2514\u2500\u2500 base_trainer.py \u2502 \u251c\u2500\u2500 data_loader/ - anything about data loading goes here \u2502 \u2514\u2500\u2500 data_loaders.py \u2502 \u251c\u2500\u2500 data/ - default directory for storing input data \u2502 \u251c\u2500\u2500 model/ - models, losses, and metrics \u2502 \u251c\u2500\u2500 model.py \u2502 \u251c\u2500\u2500 metric.py \u2502 \u2514\u2500\u2500 loss.py \u2502 \u251c\u2500\u2500 saved/ \u2502 \u251c\u2500\u2500 models/ - trained models are saved here \u2502 \u2514\u2500\u2500 log/ - default logdir for tensorboard and logging output \u2502 \u251c\u2500\u2500 trainer/ - trainers \u2502 \u2514\u2500\u2500 trainer.py \u2502 \u251c\u2500\u2500 logger/ - module for tensorboard visualization and logging \u2502 \u251c\u2500\u2500 visualization.py \u2502 \u251c\u2500\u2500 logger.py \u2502 \u2514\u2500\u2500 logger_config.json \u2502 \u2514\u2500\u2500 utils/ - small utility functions \u251c\u2500\u2500 util.py \u2514\u2500\u2500 ...","title":"Folder Structure"},{"location":"#usage","text":"$ git clone https://github.com/mheriyanto/play-with-torch.git $ cd play-with-torch $ python3 inference.py --source /dev/video0","title":"Usage"},{"location":"#to-do","text":"[ ] Implement Unit-Test: Test-Driven Development (TDD)","title":"TO DO"},{"location":"#credit-to","text":"Share PyTorch binaries built for Raspberry Pi","title":"Credit to"},{"location":"#reference","text":"Yunjey Choi - PyTorch Tutorial for Deep Learning Researchers here Victor Huang - PyTorch Template Project ( here )","title":"Reference"},{"location":"config_file_detail/","text":"NanoDet Config File Analysis NanoDet using yacs to read yaml config file. Saving path save_dir: PATH_TO_SAVE Change save_dir to where you want to save logs and models. If path not exist, NanoDet will create it. Model model: arch: name: OneStageDetector backbone: xxx fpn: xxx head: xxx Most detection model architecture can be devided into 3 parts: backbone, task head and connector between them(e.g. FPN, BiFPN, PAN...). Backbone backbone: name: ShuffleNetV2 model_size: 1.0x out_stages: [2,3,4] activation: LeakyReLU with_last_conv: False NanoDet using ShuffleNetV2 as backbone. You can modify model size, output feature levels and activation function. Moreover, NanoDet provides other lightweight backbones like GhostNet and MobileNetV2 . You can also add your backbone network by importing it in nanodet/model/backbone/__init__.py . FPN fpn: name: PAN in_channels: [116, 232, 464] out_channels: 96 start_level: 0 num_outs: 3 NanoDet using modified PAN (replace downsample convs with interpolation to reduce amount of computations). in_channels : a list of feature map channels extracted from backbone. out_channels : out put feature map channel. Head head: name: NanoDetHead num_classes: 80 input_channel: 96 feat_channels: 96 stacked_convs: 2 share_cls_reg: True octave_base_scale: 8 scales_per_octave: 1 strides: [8, 16, 32] reg_max: 7 norm_cfg: type: BN loss: name : Task head class name num_classes : number of classes input_channel : input feature map channel feat_channels : channel of task head convs stacked_convs : how many conv blocks use in one task head share_cls_reg : use same conv blocks for classification and box regression octave_base_scale : base box scale scales_per_octave : anchor free model only have one base box, default value 1 strides : down sample stride of each feature map level reg_max : max value of per-level l-r-t-b distance norm_cfg : normalization layer setting loss : adjust loss functions and weights Data data: train: name: coco img_path: coco/train2017 ann_path: coco/annotations/instances_train2017.json input_size: [320,320] keep_ratio: True pipeline: val: ..... In data you need to set your train and validate dataset. name : Dataset format name. You can create your own dataset format in nanodet/data/dataset . input_size : [width, height] keep_ratio : whether to maintain the original image ratio when resizing to input size pipeline : data preprocessing and augmentation pipeline Device device: gpu_ids: [0] workers_per_gpu: 12 batchsize_per_gpu: 160 gpu_ids : CUDA device id. For multi-gpu training, set [0, 1, 2...]. workers_per_gpu : how many dataloader processes for each gpu batchsize_per_gpu : amount of images in one batch for each gpu schedule schedule: # resume: # load_model: YOUR_MODEL_PATH optimizer: name: SGD lr: 0.14 momentum: 0.9 weight_decay: 0.0001 warmup: name: linear steps: 300 ratio: 0.1 total_epochs: 70 lr_schedule: name: MultiStepLR milestones: [40,55,60,65] gamma: 0.1 val_intervals: 10 Set training schedule. resume : whether to restore last training process load_model : path to trained weight optimizer : Support all optimizer provided by pytorch. You should adjust the lr with batch_size. Following linear scaling rule in paper Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour warmup : Warm up your network before training. Support constant , exp and linear three types of warm up. total_epochs : total epochs to train lr_schedule : please refer to pytorch lr_scheduler documentation val_intervals : epoch interval of evaluating during training Evaluate evaluator: name: CocoDetectionEvaluator save_key: mAP Currently only support coco eval. save_key : Metric of best model. Support mAP, AP50, AP75.... class_names : used in visualization","title":"NanoDet Config File Analysis"},{"location":"config_file_detail/#nanodet-config-file-analysis","text":"NanoDet using yacs to read yaml config file.","title":"NanoDet Config File Analysis"},{"location":"config_file_detail/#saving-path","text":"save_dir: PATH_TO_SAVE Change save_dir to where you want to save logs and models. If path not exist, NanoDet will create it.","title":"Saving path"},{"location":"config_file_detail/#model","text":"model: arch: name: OneStageDetector backbone: xxx fpn: xxx head: xxx Most detection model architecture can be devided into 3 parts: backbone, task head and connector between them(e.g. FPN, BiFPN, PAN...).","title":"Model"},{"location":"config_file_detail/#backbone","text":"backbone: name: ShuffleNetV2 model_size: 1.0x out_stages: [2,3,4] activation: LeakyReLU with_last_conv: False NanoDet using ShuffleNetV2 as backbone. You can modify model size, output feature levels and activation function. Moreover, NanoDet provides other lightweight backbones like GhostNet and MobileNetV2 . You can also add your backbone network by importing it in nanodet/model/backbone/__init__.py .","title":"Backbone"},{"location":"config_file_detail/#fpn","text":"fpn: name: PAN in_channels: [116, 232, 464] out_channels: 96 start_level: 0 num_outs: 3 NanoDet using modified PAN (replace downsample convs with interpolation to reduce amount of computations). in_channels : a list of feature map channels extracted from backbone. out_channels : out put feature map channel.","title":"FPN"},{"location":"config_file_detail/#head","text":"head: name: NanoDetHead num_classes: 80 input_channel: 96 feat_channels: 96 stacked_convs: 2 share_cls_reg: True octave_base_scale: 8 scales_per_octave: 1 strides: [8, 16, 32] reg_max: 7 norm_cfg: type: BN loss: name : Task head class name num_classes : number of classes input_channel : input feature map channel feat_channels : channel of task head convs stacked_convs : how many conv blocks use in one task head share_cls_reg : use same conv blocks for classification and box regression octave_base_scale : base box scale scales_per_octave : anchor free model only have one base box, default value 1 strides : down sample stride of each feature map level reg_max : max value of per-level l-r-t-b distance norm_cfg : normalization layer setting loss : adjust loss functions and weights","title":"Head"},{"location":"config_file_detail/#data","text":"data: train: name: coco img_path: coco/train2017 ann_path: coco/annotations/instances_train2017.json input_size: [320,320] keep_ratio: True pipeline: val: ..... In data you need to set your train and validate dataset. name : Dataset format name. You can create your own dataset format in nanodet/data/dataset . input_size : [width, height] keep_ratio : whether to maintain the original image ratio when resizing to input size pipeline : data preprocessing and augmentation pipeline","title":"Data"},{"location":"config_file_detail/#device","text":"device: gpu_ids: [0] workers_per_gpu: 12 batchsize_per_gpu: 160 gpu_ids : CUDA device id. For multi-gpu training, set [0, 1, 2...]. workers_per_gpu : how many dataloader processes for each gpu batchsize_per_gpu : amount of images in one batch for each gpu","title":"Device"},{"location":"config_file_detail/#schedule","text":"schedule: # resume: # load_model: YOUR_MODEL_PATH optimizer: name: SGD lr: 0.14 momentum: 0.9 weight_decay: 0.0001 warmup: name: linear steps: 300 ratio: 0.1 total_epochs: 70 lr_schedule: name: MultiStepLR milestones: [40,55,60,65] gamma: 0.1 val_intervals: 10 Set training schedule. resume : whether to restore last training process load_model : path to trained weight optimizer : Support all optimizer provided by pytorch. You should adjust the lr with batch_size. Following linear scaling rule in paper Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour warmup : Warm up your network before training. Support constant , exp and linear three types of warm up. total_epochs : total epochs to train lr_schedule : please refer to pytorch lr_scheduler documentation val_intervals : epoch interval of evaluating during training","title":"schedule"},{"location":"config_file_detail/#evaluate","text":"evaluator: name: CocoDetectionEvaluator save_key: mAP Currently only support coco eval. save_key : Metric of best model. Support mAP, AP50, AP75.... class_names : used in visualization","title":"Evaluate"}]}